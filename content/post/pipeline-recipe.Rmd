---
title: "Recipe for R data pipelines"
author: Nick Griffiths
date: 2020-04-09
categories: Programming
---

```{r, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

There are a lot of different aspects to processing a raw dataset for analysis. It helps to tackle things in order and keep similar steps together.

## Before starting, make a plan

Answer these questions:

- What are the input datasets?
- What are the output datasets?
- Will you need any intermediates?

Then describe each dataset. First, what is the unit of observation? One row will contain one observation. It could be one row per person, or per time point, or per group.

Second, what variables will be included? It doesn't make sense to remove variables that were in the input dataset unless they get in the way of later reshaping or calculations. Keep as much as possible.

## Make data accessible (ASAP)

### 1. Rename columns

Make all names short and informative. With `tidyverse`, the convention is to use underscore names (`col_one`).

### 2. Discard data that gets in the way

Use `filter` here and try not to use it anywhere else. That way you can easily find all the filters in one place for later reference.

### 3. Reshape and join datasets

If there are multiple sources of data, and all of them have the same unit of observation, it's best to merge them as soon as possible.

Often, the input dataset doesn't have a row for each unit of observation. For example, you might need data with a row per participant but the input dataset has responses spread out across rows. This is the best time to pivot the data into the correct format.

## Prep for analysis

### 4. Recode

The point of recoding is to put data into a format that can be used in calculations or analysis.

Some guidelines:

- The best format for data depends on its purpose (filter, math, viz, modeling)
- Format the same information consistently
- Combine columns when possible (e.g. combine dates and times)
- Group-wise information should be recycled through the group

The information should not change, so the variables themselves can be overwritten.

### 5. Perform calculations

Use the following framework:

```{r results="hide"}
suppressMessages(library(dplyr))

mtcars %>%
  group_by(vs) %>%
  mutate(newhp = mean(hp[gear == 3]))
```

Here, `hp[gear == 3]` operates over observations of `hp` when `gear` is 3, and `mutate` calculates each mean within the groups of `vs`.

The simple combination of `group_by`, `mutate`, and `[` can handle a surprisingly wide variety of tasks.

Write some tests to ensure the calculation is correct.

### 6. Rearrange

If there are columns that define the observation unit, put those first.

When groups of columns are often used together, keep them in order so they can be selected with `select(first:last)`.

## R tools

Examples with the `nycflights13` data

```{r}
library(nycflights13)
```

### Testing

Many testing packages are available, including `tinytest`, `assertr`, and `testthat`.

```{r}
library(tinytest)
library(checkmate) # extends tinytest

mean_delays <- flights %>%
  mutate(mean_del = mean(dep_delay, na.rm = T))

expect_double(mean_delays$mean_del, any.missing = F)
```

### Viewing data

Summary stats:

```{r}
print(skimr::skim(flights[1:3])) # Better summary (print not needed)
```

Head of all variables:

```{r}
tibble::glimpse(flights) # Better head
```

### Getting help

Docs and examples:

```{r}
?sum

example(sum)
```
