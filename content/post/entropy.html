---
title: "Entropy and KL divergence"
date: 2020-07-03
author: Nick Griffiths
categories: Statistics
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="shannon-entropy" class="section level2">
<h2>Shannon Entropy</h2>
<p>Shannon entropy is a property of a probability distribution.</p>
<p>For a discrete distribution with only one event P(x) = 1, entropy is zero:</p>
<pre class="r"><code>library(entropy)

entropy(1)
## [1] 0</code></pre>
<p>Entropy is proportional to the number of binary variables (bits) we need to capture all (equally likely) discrete outcomes:</p>
<pre class="r"><code>entropy(c(&quot;1&quot; = 1,&quot;0&quot; = 1)) # one binary variable
## [1] 0.6931472

entropy(c(&quot;11&quot; = 1,&quot;00&quot; = 1, &quot;10&quot; = 1, &quot;01&quot; = 1)) # two variables
## [1] 1.386294

entropy(c(&quot;111&quot; = 1, &quot;110&quot; = 1, &quot;101&quot; = 1, &quot;100&quot; = 1,
          &quot;011&quot; = 1, &quot;010&quot; = 1, &quot;001&quot; = 1, &quot;000&quot; = 1)) # 3 variables
## [1] 2.079442</code></pre>
<p>If we have two events and weight one of them more, entropy decreases. This reflects the fact that as we increase the weight, we approach the single event distribution with no entropy:</p>
<pre class="r"><code>entropy(c(1,1))
## [1] 0.6931472

entropy(c(1,3))
## [1] 0.5623351

entropy(c(1,10))
## [1] 0.3046361

entropy(c(1,10^6))
## [1] 1.48155e-05</code></pre>
<p>Similarly, for continuous distributions, the entropy increases with flatter, wider, higher variance distributions:</p>
<pre class="r"><code>entropy(dnorm(-10:10))
## [1] 1.418938

entropy(dnorm(-10:10, sd = 100))
## [1] 3.044521</code></pre>
</div>
<div id="kullbackleibler-divergence" class="section level2">
<h2>Kullback–Leibler divergence</h2>
<p>KL divergence describes differences in two distributions based on their entropies. The KL divergence of a distribution with itself is 0:</p>
<pre class="r"><code>KL.empirical(c(1,1), c(1,1))
## [1] 0</code></pre>
<p>It is the entropy carried by the <em>additional</em> binary variables (bits) needed by the higher-entropy distribution to fully represent the outcomes:</p>
<pre class="r"><code>dist1 &lt;- c(&quot;11&quot; = 1,&quot;00&quot; = 1, &quot;10&quot; = 1, &quot;01&quot; = 1) # 2 binary variables
dist2 &lt;- c(&quot;111&quot; = 1, &quot;110&quot; = 1, &quot;101&quot; = 1, &quot;100&quot; = 1,
          &quot;011&quot; = 1, &quot;010&quot; = 1, &quot;001&quot; = 1, &quot;000&quot; = 1) # 3 binary variables

KL.empirical(dist1, dist2)
## [1] 0.6931472

entropy(c(&quot;1&quot; = 1, &quot;0&quot; = 1)) # entropy of one binary variable
## [1] 0.6931472</code></pre>
</div>
<div id="cross-entropy" class="section level2">
<h2>Cross Entropy</h2>
<p>Cross entropy is closely related to KL divergence and also contrasts two distributions. It is just the KL divergence plus the entropy of the lower-entropy distribution.</p>
<p>When the higher-entropy distribution (d2 below) has equally-weighted outcomes, cross-entropy is the same as its entropy regardless of the other distribution:</p>
<pre class="r"><code>d1 &lt;- c(1,3,2,1,1)

d2 &lt;- c(1,1,1,1,1)

entropy(d1)
## [1] 1.494175
entropy(d2)
## [1] 1.609438

entropy(d1) + KL.empirical(d1,d2)
## [1] 1.609438</code></pre>
<p>Otherwise the cross-entropy could be either higher or lower. For example, if the lower-entropy distribution removes a higher-weighted value, cross-entropy exceeds entropy:</p>
<pre class="r"><code>d1 &lt;- c(1,0,2,1,1)
entropy(d1)
## [1] 1.332179

d2 &lt;- c(1,2,2,1,1)

entropy(d2)
## [1] 1.549826
entropy(d1) + KL.empirical(d1,d2)
## [1] 1.668651</code></pre>
<p>If it removes a lower-weighted value, cross-entropy is lower than entropy:</p>
<pre class="r"><code>d1 &lt;- c(1,2,2,0,1)
entropy(d1)
## [1] 1.329661

d2 &lt;- c(1,2,2,1,1)

entropy(d2)
## [1] 1.549826
entropy(d1) + KL.empirical(d1,d2)
## [1] 1.483812</code></pre>
<p>Finally, cross-entropy has an important connection with the likelihood. If d1 is a model we’ve built, and d2 is the process we are modeling, then the cross-entropy is a scaled equivalent of the likelihood function. We can do maximum likelihood estimation by minimizing the cross-entropy.</p>
</div>
